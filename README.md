Here's the **README.md** file for your Movie Data ETL Pipeline project:

## Author

**Randal Carr**  

---

# Movie Data ETL Pipeline with Airflow & PostgreSQL (Dockerized)

This is a fully Dockerized data engineering pipeline that extracts movie data from a public API, stores raw data in PostgreSQL, transforms it into silver and gold tables, and orchestrates the entire process with Apache Airflow. The project also includes structured logging to track ETL stages in detail.

---

âš ï¸ Disclaimer on Data Coverage
Due to limitations of the free tier of the TMDB API, this project can only retrieve a maximum of 500 pages of movie data (20 movies per page), which limits the dataset to approximately 10,000 movies. As a result, this dataset does not represent the full TMDB movie catalog, and some titles may be missing.

Future improvements will include a mechanism to incrementally insert new movies on a daily basis using TMDBâ€™s daily export of new movie IDs, enabling broader and more up-to-date coverage over time.

# ğŸ¬ Movie Analytics Dashboard

ğŸ‘‰ [**Explore the interactive Top Movies chart on Tableau Public**](https://public.tableau.com/views/Movies-ETL-Pipeline-Dashboard/Sheet1)

Get insights on movie ratings, genres, and yearly trends through an interactive Tableau dashboard. Hover over each bar to see vote counts, genres, and release dates. More visuals to come!

ğŸ‘‰ [**Explore the interactive Top Movies ROI chart on Tableau Public**](https://public.tableau.com/app/profile/randal.carr/viz/TopMoviesROI/Sheet1?publish=yes)

Get insights on movie budgets, revenues, and ROI through an interactive Tableau dashboard. Hover over each point to see movie titles, budgets, revenues, and profitability. 
---



## Project Structure
```
movie_data_pipeline/
â”‚
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ init_schema.py              # Creates DB schema on Airflow start
â”‚   â””â”€â”€ movie_etl_dag.py            # Airflow DAG definition
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ extract_movies.py           # Extract movie metadata
â”‚   â”œâ”€â”€ extract_genres.py           # Extract genre data
â”‚   â”œâ”€â”€ extract_budget_revenue.py   # Extract budget and revenue data
|   â”œâ”€â”€ extract_cast_and_crew.py    # get top cast and crew data by movie id
â”‚   â”œâ”€â”€ transform_silver_layer.py   # Clean and deduplicate raw data
â”‚   â”œâ”€â”€ transform_gold_layer.py     # Enrich and finalize analytics-ready data
â”‚   â”œâ”€â”€ logger.py                   # Custom logger for ETL steps
â”‚   â””â”€â”€ tableau/
â”‚       â””â”€â”€ hyper_exports/          # CSVs for Tableau dashboards
â”‚           â”œâ”€â”€ avg_rating_by_lang.csv
â”‚           â”œâ”€â”€ gold_top_movies.csv
â”‚           â””â”€â”€ yearly_counts.csv
â”‚
â”œâ”€â”€ ml/
â”‚   â”œâ”€â”€ predict_genre.py 
â”‚   â”œâ”€â”€ preprocess_text.py          # Preprocess text for overview column for machine learning training
â”‚   â”œâ”€â”€ train_genre_multilabel.py   # Train the model
â”‚   â”œâ”€â”€ utils.py
â”œâ”€â”€ sql/
â”‚   â””â”€â”€ create_table.sql            # PostgreSQL table creation script
â”‚
â”œâ”€â”€ db/
â”‚   â””â”€â”€ db_connector.py             # DB connection helper
â”‚
â”œâ”€â”€ logs/                           # ETL & Airflow logs
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ extract.log
â”‚   â”‚   â”œâ”€â”€ transform.log
â”‚   â”‚   â””â”€â”€ load.log
â”‚   â”œâ”€â”€ scheduler/                  # Airflow scheduler logs
â”‚   â”œâ”€â”€ dag_id=movie_data_etl/     # Logs by DAG run and task
â”‚   â””â”€â”€ dag_processor_manager/     # DAG processor logs
â”‚
â”œâ”€â”€ jars/
â”‚   â””â”€â”€ postgresql-42.7.1.jar       # JDBC driver for PostgreSQL
â”‚
â”œâ”€â”€ docker-compose.yml             # Docker config for Airflow, Postgres, etc.
â”œâ”€â”€ Dockerfile                     # Custom Airflow image with dependencies
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env                           # Environment variables (DB, secrets)
â”œâ”€â”€ README.md                      # Project documentation
â””â”€â”€ command_line_notes.md          # Helpful CLI commands during development
```

---

## Pipeline Stages
[API] â†’ [Extract Scripts] â†’ [PostgreSQL Raw Tables] â†’ [Silver & Gold Tables] â†’ [Tableau/ML Model]

### Extract

* Pulls movie data from a public API or scrape source.

### Load Raw

* Inserts unprocessed data into the `raw_movies` table in PostgreSQL.

### Transform Silver

* Cleans and standardizes the data into `silver_movies`.

### Transform Gold

* Aggregates insights into `gold_movies`.

### Orchestration

* Apache Airflow manages and schedules all tasks.

### Machine Learning

  Preprocesses plot summaries and trains a multi-label classification model to predict movie genres from the overview text.

  - `preprocess_text.py`  
    Cleans and tokenizes the `overview` text column to prepare it for model training.

  - `train_genre_multilabel.py`  
    Trains a multi-label classifier to predict genres based on preprocessed plot summaries.

  - `predict_genere.py`  
    Applies the trained model to generate genre predictions.

  - `utils.py`  
    Contains shared helper functions used across the ML pipeline.

---

## Technologies Used

* **Python 3.10+**
* **PostgreSQL**
* **Apache Airflow 2.7+**
* **Docker & Docker Compose**
* **Requests** 
* **Pandas** (for data wrangling)
* **Custom Logging** (per ETL step)

---

## Requirements

* `apache-airflow==2.7.1`
* `psycopg2-binary`
* `requests`
* `beautifulsoup4`
* `pandas`
* `python-dotenv`

---

## Setup Instructions (Dockerized)

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/movie-data-pipeline.git
cd movie-data-pipeline
```

### 2. Create `.env`

Create an `.env` file in the root of the project directory with the following content:

```plaintext
POSTGRES_USER=airflow
POSTGRES_PASSWORD=airflow
POSTGRES_DB=movies
POSTGRES_HOST=postgres
POSTGRES_PORT=5432
```
## ğŸ” Managing Secrets and Configuration with Airflow Variables

This project uses **Apache Airflow Variables** to securely manage secrets and configuration values at runtime. For example, the TMDB API key is accessed within DAGs or Python scripts using Airflowâ€™s built-in `Variable.get()` method:

```python
from airflow.models import Variable

TMDB_API_KEY = Variable.get("MY_API_KEY")

âœ… Setting the Variable in Airflow
  Via Airflow UI:

  Go to Admin â†’ Variables
  Click the "+" button to add a new variable
  Set Key to MY_API_KEY
  Set Value to your actual TMDB API key
### 3. Build and Start Services

Run the following command to build and start the Docker services:

```bash
docker-compose up --build
```

This starts:

## Services Overview

- **Airflow Webserver**  
  Access the Airflow UI at: [http://localhost:8080](http://localhost:8080)

- **Airflow Scheduler**  
  Responsible for scheduling and triggering DAG tasks.

- **PostgreSQL Database**  
  Runs on `localhost:5432`. Used as the main data warehouse for storing:
  - `raw_movies`
  - `raw_genres`
  - `raw_finances`
  - `movies_silver`
  - `movies_gold`
  - `ml_genre_predictions`
  - `gold_avg_rating_by_language`
  - `gold_top_movies`
  - `gold_yearly_counts`


- **pgAdmin**  
  Access pgAdmin UI at: [http://localhost:5050](http://localhost:5050)  
  Default login: `admin@admin.com / [your password in .env or Docker secrets]`


### Default Airflow credentials:

* **Username**: airflow
* **Password**: airflow

### 4. Initialize Airflow

Run the following commands to initialize the Airflow database and create the default user:

```bash
docker-compose exec airflow-webserver airflow db init
docker-compose exec airflow-webserver airflow users create \
  --username airflow --password airflow \
  --firstname Air --lastname Flow --role Admin --email airflow@example.com
```

### 5. Create Database Tables

Run the scripts in the `sql/` directory using a tool like **DBeaver**, **pgAdmin**, or with a DAG task to auto-init the schema in PostgreSQL.

---

## Airflow DAG Tasks

- **`extract_movies`**  
  Fetches raw movie metadata from the TMDB API and stores it in the `raw_movies` table.

- **`extract_genres`**  
  Retrieves the latest genre mappings from TMDB and stores them in the `raw_genres` table.

- **`extract_financials`**  
  Collects budget and revenue details for each movie and saves them to the `raw_finances` table.

- **`transform_to_silver`**  
  Cleans, normalizes, and enriches the raw data (e.g., mapping genre IDs to names) and writes the results to the `movies_silver` table.

- **`transform_movies_gold_task`**  
Generates curated gold-layer analytics tables based on cleaned movie data, enabling downstream reporting and insights.

  - `gold_avg_rating_by_language`  
  Provides the average movie rating grouped by the primary spoken language. Useful for analyzing how movies perform across different linguistic audiences.

  - `gold_top_movies`  
  Contains the top-rated movies, typically filtered by vote count to ensure quality. Highlights the most critically acclaimed or popular films in the dataset.

  - `gold_yearly_counts`  
  Summarizes the number of movies released each year. Useful for visualizing historical trends in movie production volume over time.


---

## Logging

Each ETL step uses a shared `logger.py` utility for structured logging into the `logs/etl/` directory:

* `logs/etl/extract.log`
* `logs/etl/transform.log`
* `logs/etl/load.log`

These logs help you monitor step-by-step progress, errors, and runtime metrics. The `logs/` folder is mounted as a Docker volume so logs persist on the host machine even after container shutdown.

### Sample log entry:

```plaintext
2025-05-05 12:00:00 - INFO - Starting extraction task...
```
---
### ğŸ¯ Genre Prediction Model
- **Model:** Logistic Regression (multi-label)
- **Input:** Tokenized `overview` text
- **Target:** One-hot encoded genres
- **Performance:** ~XX% accuracy on validation set

## Improvements & Ideas

* Add **data validation** (using tools like **Great Expectations** or custom validation rules).
* Dockerize for **production** (multi-container deployment).
* Integrate a **dashboard** (using **Streamlit** or **Superset**) to visualize gold-layer insights.
* Schedule with **cron** + Airflow variables.
* Add **unit tests** for `src/` logic.

---

## License

This project is licensed under the **MIT License**.

